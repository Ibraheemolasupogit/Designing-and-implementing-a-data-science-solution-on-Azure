# ðŸ§  Designing and Implementing Data Science Solutions on Azure  

This repository contains **practical lab exercises and implementations** completed through the **DP-100 learning path**.  
It demonstrates **end-to-end Azure Machine Learning workflows** for data preparation, model training, deployment, and monitoring within the **Azure ecosystem**.  

ðŸ“‚ **Explore the Project:**  
- View all lab notebooks in the **[Projects](./Projects)** folder  
- See lab progress and results in the **[Lab Screenshots](./lab_screenshots)** folder  

## ðŸ§© Overview  
Hands-on experience building and operationalizing data science solutions using:

- **Azure Machine Learning Workspace** for managing resources and experiments  
- **Azure ML Designer** for visual model design and deployment  
- **Python SDK & Jupyter Notebooks** for custom model development  
- **MLflow** for experiment tracking and model versioning  
- **Responsible AI Dashboard** for fairness and interpretability  
- **Managed Online and Batch Endpoints** for production deployment  
- **Prompt Engineering and Model Catalog** for Generative AI exploration  

---

## ðŸ”’ Prerequisites  

- An active **Azure subscription** with administrative permissions  
- Access to an **Azure Machine Learning workspace**  
- Familiarity with **Python, scikit-learn, and Azure ML Studio**  
- *Optional:* Integration with **Azure Databricks** or **Azure Data Factory** for data preparation and automation  

---

## **Data Preparation & Workspace Setup**  
Configured Azure Machine Learning workspace and explored developer tools for workspace interaction.

- Explore the Azure Machine Learning workspace  
- Set up data assets and compute clusters  
- Use notebooks to interact with workspace resources  

Established a reproducible environment for end-to-end data science workflows.  

---

## **Model Training & Optimization**  
Developed and trained models using Designer, AutoML, and custom scripts.

- Train models using Azure ML Designer  
- Run training scripts as command jobs  
- Perform hyperparameter tuning with sweep jobs  
- Track model performance using MLflow  

Implemented automation and version control for training and optimization processes.  

---

## **Experiment Tracking & Model Registry**  
Monitored experiments and registered best-performing models in the Azure ML registry.

- Track model metrics and parameters using MLflow  
- Register trained models for reuse  
- Manage model lineage and version history  

Ensured consistency, reproducibility, and governance across ML workflows.  

---

## **Deployment & Inference**  
Operationalized models for real-time and batch scoring scenarios.

- Deploy models to **Managed Online Endpoints**  
- Configure **Batch Endpoints** for large-scale inference  
- Test and validate REST API responses  

Enabled scalable and secure production deployment pipelines.  

---

## **Responsible AI & Model Evaluation**  
Applied Responsible AI practices for transparency and fairness in model predictions.

- Create and explore the **Responsible AI Dashboard**  
- Evaluate model explainability and fairness metrics  
- Interpret predictions and ensure ethical AI practices  

Integrated Responsible AI frameworks into the end-to-end ML lifecycle.  

---

## **Generative AI & Prompt Engineering**  
Extended traditional ML workflows to Generative AI exploration using Azure OpenAI.

- Compare language models from the **Azure Model Catalog**  
- Explore **Prompt Engineering with Prompty**  
- Evaluate model responses for performance and safety  

Demonstrated integration of LLM-based reasoning and prompt design into Azure ML workflows.  

---

## Skills  

- Designed reproducible **data science workflows** on Azure  
- Applied **MLOps practices** for training, tracking, and deployment  
- Built and deployed **Responsible AI** solutions  
- Integrated **Generative AI** with prompt engineering techniques  
- Strengthened understanding of **end-to-end ML lifecycle management** in Azure  
